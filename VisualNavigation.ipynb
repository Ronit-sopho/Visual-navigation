{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "import cv2\n",
    "import multiprocessing as mp\n",
    "import sys, os, threading\n",
    "\n",
    "from segment import road_segmentation\n",
    "from ipmdistance import getDistance\n",
    "\n",
    "from skimage import filters\n",
    "from skimage.feature import corner_harris, corner_peaks, CENSURE\n",
    "from skimage.color import rgb2gray\n",
    "from motion import pyramid_lucas_kanade\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 12)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn on the detection pipeline\n",
    "from detect import *\n",
    "from utils import convert_to_original_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset previous initiation\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate Detection Object\n",
    "detection_object = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to load images into memory\n",
    "def load_images(path):\n",
    "\n",
    "    getFrames = []\n",
    "    for imgs in sorted(os.listdir(path)):\n",
    "        getFrames.append(Image.open(os.path.join(path, imgs)))\n",
    "    return getFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the bounding boxes for detected objects and initiate trackers for them\n",
    "\n",
    "def runDetection(frame):\n",
    "    \n",
    "    pilImage = Image.fromarray(frame)\n",
    "    outputImage, bounding_boxes = detection_object.infer(pilImage)\n",
    "    outputFrame = np.asarray(outputImage)\n",
    "    \n",
    "    # Initiate trackers for the bounding boxes\n",
    "    detectedObjects = []\n",
    "    results = []\n",
    "    trackers = []\n",
    "    \n",
    "    for cls, bboxes in bounding_boxes.items():\n",
    "        for box, score in bboxes:\n",
    "            if(np.all(box>0)):\n",
    "                detectedObjects.append(box)\n",
    "    \n",
    "    ntrackers = len(detectedObjects)\n",
    "    for r in range(ntrackers):\n",
    "        trck = cv2.TrackerMIL_create()\n",
    "        x0,y0,x1,y1 = tuple(detectedObjects[r].astype(int))\n",
    "        if x1-x0<=500 and y1-y0<=500:\n",
    "            try:\n",
    "                trck.init(frame, (x0,y0,x1-x0, y1-y0))\n",
    "                trackers.append(trck)\n",
    "                results.append((x0,y0,x1-x0,y1-y0))\n",
    "            except:\n",
    "                print('Error Encountered')\n",
    "                continue\n",
    "    \n",
    "    return outputFrame, trackers, results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to track an object in consecutive frames\n",
    "def SingleTracker(trackerObject, vid_frame, output):\n",
    "\n",
    "    try:\n",
    "        ret, bbox = trackerObject.update(vid_frame)\n",
    "    except:\n",
    "        pass\n",
    "    if(ret):\n",
    "        output.put(bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multiple Instance Tracking with threading in python\n",
    "def parallelTracking(frame, trackers, output):\n",
    "    \n",
    "    threads = [threading.Thread(target=SingleTracker, args=(trck, frame, output,)) for trck in trackers]\n",
    "    for t in threads: t.start()\n",
    "    for t in threads: t.join()\n",
    "    \n",
    "    results = [output.get() for t in threads]\n",
    "    for t in trackers: del t\n",
    "    \n",
    "    for box in results:\n",
    "        p1 = (int(box[0]), int(box[1]))\n",
    "        p2 = (int(box[0]+box[2]), int(box[1]+box[3]))\n",
    "        cv2.rectangle(frame, p1, p2, (0,0,200), 2, 1)\n",
    "    \n",
    "    return frame, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optical flow for navigation\n",
    "\n",
    "def motionVectors(detectedObjects, currentFrame, nextFrame):\n",
    "    \n",
    "    censure = CENSURE()\n",
    "    keypoints = np.array([]).reshape(-1,2)\n",
    "    \n",
    "    for region in detectedObjects:\n",
    "        x0,y0,w,h = region\n",
    "        roi = rgb2gray(currentFrame[int(y0-5):int(y0+h+5), int(x0-5):int(x0+w+5)])\n",
    "        censure.detect(roi)\n",
    "        kps = censure.keypoints\n",
    "        kps[:,1]+=int(x0)\n",
    "        kps[:,0]+=int(y0)\n",
    "        keypoints = np.append(keypoints, kps, axis=0)\n",
    "    \n",
    "    flow_vectors = pyramid_lucas_kanade(rgb2gray(currentFrame), rgb2gray(nextFrame), keypoints, window_size=9)\n",
    "#     print(flow_vectors)\n",
    "    for y,x,vy,vx in np.hstack((keypoints,flow_vectors)):\n",
    "        cv2.arrowedLine(nextFrame, (int(x),int(y)), (int(x+vx),int(y+vy)), (25,32,33), 3)\n",
    "    \n",
    "    return nextFrame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define video handle\n",
    "cap = cv2.VideoCapture(\"indiavideo.mp4\")\n",
    "\n",
    "# Counter for frames\n",
    "count = 0\n",
    "\n",
    "try:\n",
    "    while(True):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            cap.release()\n",
    "            print(\"Released Video Resource\")\n",
    "            break\n",
    "        \n",
    "        H,W,_ = frame.shape\n",
    "        orgFrame = frame.copy()\n",
    "        print(count)\n",
    "        \n",
    "        # Run YOLO detection for every 10th frame\n",
    "        if count%10==0:\n",
    "            frame, trackers, results = runDetection(frame)\n",
    "#             print('detected')\n",
    "            output = mp.Queue()\n",
    "        else:\n",
    "            frame, results = parallelTracking(frame, trackers, output)\n",
    "#             print('Tracked')\n",
    "        \n",
    "        output = mp.Queue()\n",
    "        thread_distance = threading.Thread(target = getDistance, args=(frame, results, output,))\n",
    "        thread_segment = threading.Thread(target = road_segmentation, args = (frame, output,))\n",
    "        threads = [thread_distance, thread_segment]\n",
    "        for t in threads: t.start()\n",
    "        for t in threads: t.join()\n",
    "        res = [output.get() for t in threads]\n",
    "        for r in res:\n",
    "            if type(r)==type({}):\n",
    "                distance_to_objects = r\n",
    "            else:\n",
    "                frame = r\n",
    "        \n",
    "        for k in distance_to_objects.keys():\n",
    "            mid_x = int(distance_to_objects[k][0]+distance_to_objects[k][2]/2)\n",
    "            mid_y = int(distance_to_objects[k][1]+distance_to_objects[k][3])\n",
    "            cv2.putText(frame, str(k)[:4]+' meters',(mid_x,mid_y+3), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 7, cv2.LINE_AA)\n",
    "        \n",
    "        if count==0:\n",
    "            previousFrame = orgFrame\n",
    "        if count>=1:\n",
    "            frame = motionVectors(results, previousFrame, frame)\n",
    "            previousFrame = orgFrame\n",
    "        frame = cv2.resize(frame, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)\n",
    "        count+=1\n",
    "        cv2.imshow(\"OUTPUT\", frame)\n",
    "        k = cv2.waitKey(1) & 0xff\n",
    "        if(k==27):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "#         cv2.imwrite(\"output/frames/frame_{}.png\".format(count), frame)\n",
    "except KeyboardInterrupt:\n",
    "    cap.release()\n",
    "    clear_output()\n",
    "    print(\"Released Video Resource due to interrupt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
